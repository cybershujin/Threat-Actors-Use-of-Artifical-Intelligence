# Threat Actors List

This github is an attempt to organize known use of artificial intelligence by cyber threat actors and to map and track those techniques. In this scenario we are focusing on cyber threat attacks that are being faciliated in some way by threat actors using artifical intelligence. This does not include political influence campaigns or mis/dis/mal information campaigns. It does include some fraud related cases, but I attempt to keep the focus on fraud activities that we would see in traditional campaigns but enhanced with AI.

It is worth specifically stating that in many cases defenders cannot confirm whether a threat actor used AI unless: (a) the reporting organization is looking at the use of their own AI tools such as Microsoft and OpenAI's reporting or (b) the actor decides to use AI tools available on the already-compromised endpoint. For this reason many reports on attacks or campaigns that are fascilated by AI are, when you read carefully, actually done by researchers. For this reason I am remaining focused in this project *only* on what confirmed reports we have on threat actor's actual use.

There have been relatively few individuals on criminal forums and telegrams actively discussing generative AI use. Some users have marketed alleged generative AI tools used for malicious purposes. See [Dark LLMs and Blackhat GPTs](https://github.com/cybershujin/Threat-Actors-Use-of-Artifical-Intelligence/blob/main/Dark%20LLMs%20and%20Malicious%20AIs.MD). 

There has been secondary effect studies such as the significant increase observed in phishing since ChatGPT became widely available, reported as 1,265% increase between November 2022 to Jan 2023 by [SlashNext](https://www.prnewswire.com/news-releases/slashnexts-2023-state-of-phishing-report-reveals-a-1-265-increase-in-phishing-emails-since-the-launch-of-chatgpt-in-november-2022--signaling-a-new-era-of-cybercrime-fueled-by-generative-ai-301971557.html). However, Slashnext and many other reports authored in this area are from vendors offering solutions in these security spaces and thus should be examined accordingly for bias.

Generally speaking most popular and widely accessible AIs and LLMs have significant safeguards so they are designed not to add or enable malicious or criminal activities. However, this requires "jailbreaking" or otherwise "attacking" the AI/LLM itself which is not covered here, but is on my [other repo](https://github.com/cybershujin/AI-and-ML-for-Cybersecurity).

## Scope

This is not for:

• Threat actors attacking AI / ML / LLMs

• Researchers attacking or finding exploits or possible uses AI by threat actors

• Mis/dis/mal information campaigns using Deepfake technology

• Threats to AI users, or AI researchers
For those items, some of it is in my other repo here [AI and ML for Cybersecurity](https://github.com/cybershujin/AI-and-ML-for-Cybersecurity) along with some tools for cybersecurity professionals.

Notes:
<br>
**UnSub or Multiple UnSub - (sourcename)** used to refer to unnamed subjects, or multiple unnamed subjects mentioned in reports

These these are taken from reports listed on the [Sources page](https://github.com/cybershujin/Threat-Actors-Use-of-Artifical-Intelligence/blob/main/Sources.md) but the TTP mapping is an effort by me to map to the closest possible MITRE ATT&CK technique used. If you find an entry and believe a better TTP mapping is available, please contact me and/or comment.


| Name               | AKAs           | Brief          | TTPs | Link to Folder with reports                                                                      |
| ------------------ | -------------- | -------------------------------------------- |--------- | ------------------------------------ |
| Fancy Bear | Forest Blizzard, APT28, Strontium |  APT28 is a Russian military intelligence actor linked to GRU Unit 26165, who has targeted victims of both tactical and strategic interest to the Russian government. Microsoft assesses that Forest Blizzard operations play a significant supporting role to Russia’s foreign policy and military objectives both in Ukraine and in the broader international community. Forest Blizzard’s use of LLMs has involved research into various satellite and radar technologies that may pertain to conventional military operations in Ukraine, as well as generic research aimed at supporting their cyber operations. [Microsoft](https://www.microsoft.com/en-us/security/blog/2024/02/14/staying-ahead-of-threat-actors-in-the-age-of-ai/) | **LLM-informed reconnaissance** <br> T1592 - Gather Victim Org Information <br><br> **LLM-enhanced scripting techniques** <br> T1588 - Develop Capabilities: Tool | [APT28 aka Fancy Bear](https://github.com/cybershujin/Threat-Actors-Use-of-Artifical-Intelligence/blob/main/APT28.MD)|
| Lazarus | APT43, Emerald Sleet, Velvet Chollima, Kimsuky, TA406, Thallium | North Korean threat actor with recent operations relied on spear-phishing emails to compromise and gather intelligence from prominent individuals with expertise on North Korea. Microsoft observed Emerald Sleet impersonating reputable academic institutions and NGOs to lure victims into replying with expert insights and commentary about foreign policies related to North Korea. Emerald Sleet’s use of LLMs has been in support of this activity and involved research into think tanks and experts on North Korea, as well as the generation of content likely to be used in spear-phishing campaigns. Emerald Sleet also interacted with LLMs to understand publicly known vulnerabilities, to troubleshoot technical issues, and for assistance with using various web technologies. [Microsoft](https://www.microsoft.com/en-us/security/blog/2024/02/14/staying-ahead-of-threat-actors-in-the-age-of-ai/) | **LLM-assisted vulnerability research** <br> T1588.006 Obtain Capabilities: Vulnerabilities <br><br> **LLM-enhanced scripting techniques** <br>T1588 - Develop Capabilities: Tool <br><br> **LLM-supported social engineering** <br>T1566 - Phishing <br><br> **LLM-informed reconnaissance** <br>T1592 - Gather Victim Org Information | link coming soon|
| Imperial Kitten | Crimson Sandstorm, Yellowliderc, Tortoiseshell | Iranian threat actor assessed to be connected to the Islamic Revolutionary Guard Corps (IRGC). This actor has targeted multiple sectors, including defense, maritime shipping, transportation, healthcare, and technology. These operations have frequently relied on watering hole attacks and social engineering to deliver custom .NET malware. Prior research also identified custom Crimson Sandstorm malware using email-based command-and-control (C2) channels. The use of LLMs by Crimson Sandstorm has reflected the broader behaviors that the security community has observed from this threat actor. Interactions have involved requests for support around social engineering, assistance in troubleshooting errors, .NET development, and ways in which an attacker might evade detection when on a compromised machine. [Microsoft](https://www.microsoft.com/en-us/security/blog/2024/02/14/staying-ahead-of-threat-actors-in-the-age-of-ai/) | **LLM-supported social engineering** <br>T1566 - Phishing <br><br> **LLM-enhanced scripting techniques** <br>T1588 - Develop Capabilities: Tool <br><br> **LLM-enhanced anomaly detection evasion** <br> T1562.001 - Impair Defenses: Disable or Modify Tools |  link coming soon|
| Aquatic Panda | Charcoal Typhoon, ControlX, RedHotel, Bronze University, Red Scully, Chromium | Chinese state-affiliated threat actor with a broad operational scope. Activities have predominantly focused on entities within Taiwan, Thailand, Mongolia, Malaysia, France, and Nepal, with observed interests extending to institutions and individuals globally who oppose China’s policies. In recent operations, this actor group has been observed interacting with LLMs in ways that suggest a limited exploration of how LLMs can augment their technical operations. This has consisted of using LLMs to support tooling development, scripting, understanding various commodity cybersecurity tools, and for generating content that could be used to social engineer targets. [Microsoft](https://www.microsoft.com/en-us/security/blog/2024/02/14/staying-ahead-of-threat-actors-in-the-age-of-ai/) | **LLM-informed reconnaissance** <br>T1588.006 Obtain Capabilities: Vulnerabilities <br><br> **LLM-enhanced scripting techniques** <br>T1587 - Develop Capabilities <br><br> **LLM-refined operational command techniques** <br>TA0003 - Persistence and TA004 Privilege Escalation |  [Aquatic Panda Reports](https://github.com/cybershujin/Threat-Actors-Use-of-Artifical-Intelligence/tree/main/Aquatic%20Panda)|
| Sodium | Salmon Typhoon, Samurai Panda, Maverick Panda, APT4 | Sophisticated Chinese state-affiliated threat actor with a history of targeting US defense contractors, government agencies, and entities within the cryptographic technology sector.  This threat actor has demonstrated its capabilities through the deployment of malware, such as Win32/Wkysol, to maintain remote access to compromised systems. With over a decade of operations marked by intermittent periods of dormancy and resurgence. (Sodium's) interactions with LLMs throughout 2023 appear exploratory and suggest that this threat actor is evaluating the effectiveness of LLMs in sourcing information on potentially sensitive topics, high profile individuals, regional geopolitics, US influence, and internal affairs. This tentative engagement with LLMs could reflect both a broadening of their intelligence-gathering toolkit and an experimental phase in assessing the capabilities of emerging technologies. [Microsoft](https://www.microsoft.com/en-us/security/blog/2024/02/14/staying-ahead-of-threat-actors-in-the-age-of-ai/) |  **LLM-informed reconnaissance** <br> T1593 - Search Open Websites/Domains <br><br> **LLM-enhanced scripting techniques** <br>T1587.001 - Develop Capabilities: Malware <br><br> **LLM-refined operational command techniques**<br> T1564 - Hide Artifacts <br><br> **LLM-Aided technical translation and explanation** <br>T1593 - Search Open Websites/Domains | link coming soon|
| Unsub1 - Kaspersky | N/A | Using fake sites, they hosted “GPT chats” supposedly capable of diagnosing computer problems, making money and such. In fact, the site deployed no AI models, but only used the topic to stir interest with potential victims and make a bigger killing. One such page mimicking the Microsoft website warned visitors that their computer was infected with a Trojan. To avoid losing data, they were advised not to reboot or turn off the device until the issue was resolved. Two options were offered: call a hotline or chat with Lucy, an AI chatbot. In the second case, you had to choose a method of diagnosing the device, after which the bot said it was unable to solve the problem and recommended calling support. Naturally, professional scammers, not Microsoft engineers, were waiting at the other end of the line. *Analyst note: This sounds like [Bazacall](https://www.microsoft.com/en-us/security/blog/2021/07/29/bazacall-phony-call-centers-lead-to-exfiltration-and-ransomware/) like activity* [Source](https://securelist.com/spam-phishing-report-2023/112015/) | **Lookalike LLM** <br> T1583 Acquire Infrastructure | N/A |
| Unsub2 - Kaspersky | N/A | “Smart chatbots” were also used as “consultants” on making money online. On one site a bot pretending to be an Elon Musk design advertised investment services. After telling the new “client” that it could make them rich quickly, the robot asked about their education, income level, and investment experience. Regardless of the answers, the bot informed the client that it would do all the earning. Next, it demonstrated an amount it could offer and prompted the user to register simply by providing their contact details. Events then likely unfolded as in other similar schemes: The “AI” asked for a small fee in recognition of its intellectual abilities, and then simply vanished into the ether. [Source](https://securelist.com/spam-phishing-report-2023/112015/) | **Lookalike LLM** <br> T1583 Acquire Infrastructure <br><br> **LLM-supported social engineering** <br>T1566 - Phishing | N/A |
| Multiple Unsub - JFrog | N/A | In an "investigation of a malicious machine-learning model...The model’s payload grants the attacker a shell on the compromised machine, enabling them to gain full control over victims’ machines through what is commonly referred to as a “backdoor”...It’s crucial to emphasize that when we refer to “malicious models”, we specifically denote those housing real, harmful payloads. Our analysis has pinpointed around 100 instances of such models to date... [Source](https://jfrog.com/blog/data-scientists-targeted-by-malicious-hugging-face-ml-models-with-silent-backdoor/) | **LL Models used for backdoor deployment** <br> T1195.001 Supply Chain Compromise - Compromise Software Depedencies and Development Tools <br><br> T1059 Command and Scripting Interpreter <br> | N/A|
| baller423/goober2 | potentially star23/baller13 or just ties between them| Recently, our scanning environment flagged a particularly intriguing PyTorch model uploaded by a new user named baller423—though since deleted. The repository, baller423/goober2, contained a PyTorch model file harboring an intriguing payload....This IP address range belonging to KREOnet, which stands for “Korea Research Environment Open NETwork,” may serve as potential evidence suggesting the involvement of researchers in attempting the exploit [Source](https://jfrog.com/blog/data-scientists-targeted-by-malicious-hugging-face-ml-models-with-silent-backdoor/) | **LL Models used for backdoor deployment** <br> T1195.001 Supply Chain Compromise - Compromise Software Depedencies and Development Tools <br><br> T1059.001 Command and Scripting Interpreter: Powershell <br> |
| star23/baller13 |potentially baller423/goober2 or just ties between them| Shortly after the model was removed, we encountered further instances of the same payload with varying IP addresses. One such instance remains active: star23/baller13. It’s worth noting the similarity in the model name to the deleted user, suggesting potential ties between them. [Source](https://jfrog.com/blog/data-scientists-targeted-by-malicious-hugging-face-ml-models-with-silent-backdoor/) | **LL Models used for backdoor deployment** <br> T1195.001 Supply Chain Compromise - Compromise Software Depedencies and Development Tools <br> T1059.001 Command and Scripting Interpreter: Powershell <br> | N/A |
| Multiple Unsub - NCSC UK | N/A | Threat actors, including ransomware actors, are already using AI to increase the efficiency and effectiveness of aspects of cyber operations, such as reconnaissance, phishing and coding [Source](https://www.ncsc.gov.uk/report/impact-of-ai-on-cyber-threat) | **LLM-informed reconnaissance** <br> T1593 - Search Open Websites/Domains <br><br> **LLM-supported social engineering** <br>T1566 - Phishing <br><br> **LLM-aided development** <br>T1587 - Develop Capabilities: Tool <br><br> **LLM-aided development** <br>T1587 - Develop Capabilities: Malware <br><br> **LLM-enhanced scripting techniques** <br>T1587 - Develop Capabilities <br> | [Multiple Unsub - NCSC](https://github.com/cybershujin/Threat-Actors-Use-of-Artifical-Intelligence/tree/main/Multiple%20UnSub%20SlashNext) | N/A|
| Multiple Unsub - SlashNext | N/A | Since Q4 of 2022 when ChatGPT became widely available, there has been a 1,265% increase in malicious phishing emails, with a 967% rise in credential phishing in particular. [Source](https://www.cnbc.com/2023/11/28/ai-like-chatgpt-is-creating-huge-increase-in-malicious-phishing-email.html) | **LLM-supported social engineering** <br>T1566 - Phishing <br><br> | [Multile Unsub - SlashNext](https://github.com/cybershujin/Threat-Actors-Use-of-Artifical-Intelligence/tree/main/Multiple%20UnSub%20SlashNext)|
| UnSubs - Mandiant | N/A but described as "actors aligned with nation-states including Russia, the People's Republic of China (PRC), Iran, Ethiopia, Indonesia, Cuba, Argentina, Mexico, Ecuador, and El Salvador, along with non-state actors such as individuals on the 4chan forum." | Since 2019, Mandiant has identified numerous instances of information operations leveraging GANs, typically for use in profile photos of inauthentic personas, including by actors aligned with nation-states including Russia, the People's Republic of China (PRC), Iran, Ethiopia, Indonesia, Cuba, Argentina, Mexico, Ecuador, and El Salvador, along with non-state actors such as individuals on the 4chan forum. We judge that the publicly available nature of GAN-generated image tools such as the website thispersondoesnotexist.com has likely contributed to their frequent usage in information operations (Figure 2). Actors have also taken steps to obfuscate the AI-generated origin of their profile photos through tactics like adding filters or retouching facial features...Mandiant has noted evidence of financially motivated actors using manipulated video and voice content in business email compromise (BEC) scams, North Korean cyber espionage actors using manipulated images to defeat know your customer (KYC) requirements, and voice changing technology used in social engineering targeting Israeli soldiers. [Source](https://www.mandiant.com/resources/blog/threat-actors-generative-ai-limited)  | **DeepFake for Impersonation** <br> T1587 - Develop capabilities <br><br> | N/A |

## Appendix A: LLM-themed TTPs

**LLM-informed reconnaissance:** Employing LLMs to gather actionable intelligence on technologies and potential vulnerabilities. (CREDIT: Microsoft) <br><br>
**LLM-enhanced scripting techniques:** Utilizing LLMs to generate or refine scripts that could be used in cyberattacks, or for basic scripting tasks such as programmatically identifying certain user events on a system and assistance with troubleshooting and understanding various web technologies.(CREDIT: Microsoft)<br><br>
**LLM-aided development:** Utilizing LLMs in the development lifecycle of tools and programs, including those with malicious intent, such as malware.(CREDIT: Microsoft)<br><br>
**LLM-supported social engineering:** Leveraging LLMs for assistance with translations and communication, likely to establish connections or manipulate targets.(CREDIT: Microsoft)<br><br>
**LLM-assisted vulnerability research:** Using LLMs to understand and identify potential vulnerabilities in software and systems, which could be targeted for exploitation.(CREDIT: Microsoft)<br><br>
**LLM-optimized payload crafting:** Using LLMs to assist in creating and refining payloads for deployment in cyberattacks.(CREDIT: Microsoft)<br><br>
**LLM-enhanced anomaly detection evasion:** Leveraging LLMs to develop methods that help malicious activities blend in with normal behavior or traffic to evade detection systems.(CREDIT: Microsoft)<br><br>
**LLM-directed security feature bypass:** Using LLMs to find ways to circumvent security features, such as two-factor authentication, CAPTCHA, or other access controls.(CREDIT: Microsoft)<br><br>
**LLM-advised resource development:** Using LLMs in tool development, tool modifications, and strategic operational planning.(CREDIT: Microsoft)<br><br>
**Lookalike LLM:** Using LLMs to appear to be another legitimate LLM tool such as a chatbot. Similar to look-alike domain activity. (CREDIT: Rachel James based on Kaspersky report)<br><br>
**LL Models used for backdoor deployment:** The model’s payload grants the attacker a shell on the compromised machine, enabling them to gain full control over victims’ machines through what is commonly referred to as a “backdoor” (CREDIT: Rachel James based on JFrog report)

**DeepFake for Impersonation:** Where generative AI is used to make audio, video or photographic media used to impersonate individuals (CREDIT: Rachel James based on various reports)

