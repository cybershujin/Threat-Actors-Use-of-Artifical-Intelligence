**Threat Actors List**

This github is an attempt to organize known use of artificial intelligence by cyber threat actors and to map and track those techniques. In this scenario we are focusing on cyber threat attacks that are being facilitated in some way by threat actors using artificial intelligence. This does not include political influence campaigns or mis/dis/mal information campaigns. It does include some fraud related cases, but I attempt to keep the focus on fraud activities that we would see in traditional campaigns but enhanced with AI.

It is worth specifically stating that in many cases defenders cannot confirm whether a threat actor used AI unless: (a) the reporting organization is looking at the use of their own AI tools such as Microsoft and OpenAI's reporting or (b) the actor decides to use AI tools available on the already-compromised endpoint. For this reason many reports on attacks or campaigns that are fascinated by AI are, when you read carefully, actually done by researchers. For this reason I am remaining focused in this project *only* on what confirmed reports we have on threat actor's actual use.

There have been relatively few individuals on criminal forums (Breachedforums[.]vc, Exploit[.]in or XSS) and telegrams actively discussing generative AI use. Some users have marketed alleged generative AI tools used for malicious purposes. See [Dark LLMs and Blackhat GPTs](https://github.com/cybershujin/Threat-Actors-Use-of-Artifical-Intelligence/blob/main/Dark LLMs and Malicious AIs.MD). It is worth considering that that more technically capable and sophisticated actors are more likely to harness these types of tools. That said, there are only a small set of examples of alleged LLM outputs from these, and no posts provided evidence that LLM output was successfully used for an attack.

There has been secondary effect studies such as the significant increase observed in phishing since ChatGPT became widely available, reported as 1,265% increase between November 2022 to Jan 2023 by [SlashNext](https://www.prnewswire.com/news-releases/slashnexts-2023-state-of-phishing-report-reveals-a-1-265-increase-in-phishing-emails-since-the-launch-of-chatgpt-in-november-2022--signaling-a-new-era-of-cybercrime-fueled-by-generative-ai-301971557.html). However, Slashnext and many other reports authored in this area are from vendors offering solutions in these security spaces and thus should be examined accordingly for bias.

Generally speaking most popular and widely accessible AIs and LLMs have significant safeguards so they are designed not to add or enable malicious or criminal activities. However, this requires "jailbreaking" or otherwise "attacking" the AI/LLM itself which is not covered here, but is on my [other repo](https://github.com/cybershujin/AI-and-ML-for-Cybersecurity).

**Scope**

This is not for:

• Threat actors attacking AI / ML / LLMs

• Researchers attacking or finding exploits or possible uses AI by threat actors

• Mis/dis/mal information campaigns using Deepfake technology

• Threats to AI users, or AI researchers For those items, some of it is in my other repo here [AI and ML for Cybersecurity](https://github.com/cybershujin/AI-and-ML-for-Cybersecurity) along with some tools for cybersecurity professionals.

Notes:
 **UnSub or Multiple UnSub - (sourcename)** used to refer to unnamed subjects, or multiple unnamed subjects mentioned in reports

These these are taken from reports listed on the [Sources page](https://github.com/cybershujin/Threat-Actors-Use-of-Artifical-Intelligence/blob/main/Sources.md) but the TTP mapping is an effort by me to map to the closest possible MITRE ATT&CK technique used. If you find an entry and believe a better TTP mapping is available, please contact me and/or comment.

| **Name**                                                   | **AKAs**                                                     | **Brief**                                                    | **TTPs**                                                     | **Link to Folder with reports**                              |
| ---------------------------------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| Fancy  Bear                                                | Forest  Blizzard, APT28, Strontium                           | APT28  is a Russian military intelligence actor linked to GRU Unit 26165, who has  targeted victims of both tactical and strategic interest to the Russian  government. Microsoft assesses that Forest Blizzard operations play a  significant supporting role to Russia’s foreign policy and military  objectives both in Ukraine and in the broader international community. Forest  Blizzard’s use of LLMs has involved research into various satellite and radar  technologies that may pertain to conventional military operations in Ukraine,  as well as generic research aimed at supporting their cyber operations. [Microsoft](https://www.microsoft.com/en-us/security/blog/2024/02/14/staying-ahead-of-threat-actors-in-the-age-of-ai/) | **LLM-informed  reconnaissance**   T1592 - Gather Victim Org Information      **LLM-enhanced scripting techniques**   T1588 - Develop Capabilities: Tool | [APT28   aka Fancy Bear](https://github.com/cybershujin/Threat-Actors-Use-of-Artifical-Intelligence/blob/main/APT28.MD) |
| Lazarus                                                    | APT43,  Emerald Sleet, Velvet Chollima, Kimsuky, TA406, Thallium | North  Korean threat actor with recent operations relied on spear-phishing emails to  compromise and gather intelligence from prominent individuals with expertise  on North Korea. Microsoft observed Emerald Sleet impersonating reputable  academic institutions and NGOs to lure victims into replying with expert  insights and commentary about foreign policies related to North Korea.  Emerald Sleet’s use of LLMs has been in support of this activity and involved  research into think tanks and experts on North Korea, as well as the  generation of content likely to be used in spear-phishing campaigns. Emerald  Sleet also interacted with LLMs to understand publicly known vulnerabilities,  to troubleshoot technical issues, and for assistance with using various web  technologies. [Microsoft](https://www.microsoft.com/en-us/security/blog/2024/02/14/staying-ahead-of-threat-actors-in-the-age-of-ai/)      This account of the use of AI was also reported by Mandiant in their  23-00016993 and 24-00002657 reports. Mandiant's 2024 reporting also mentions  APT43 purchasing WormGPT in August 2023.      Reports from Feb 2024 APT43 was observed on forums discussing ChatGPT along a  topic about (toughly translated) "North Korean nuclear solution" | **LLM-assisted  vulnerability research**   T1588.006 Obtain Capabilities: Vulnerabilities      **LLM-enhanced scripting techniques**   T1588 - Develop Capabilities: Tool      **LLM-supported social engineering**   T1566 - Phishing      **LLM-informed reconnaissance**   T1592 - Gather Victim Org Information      Based on Mandiants 24-00002657 report, **DeepFake for Impersonation** (TTP  unknown, not clear how actors used generated images from MaxAi[.]me and ZMO  AI | link  coming soon                                            |
| Imperial  Kitten                                           | Crimson  Sandstorm, Yellowliderc, Tortoiseshell              | Iranian  threat actor assessed to be connected to the Islamic Revolutionary Guard  Corps (IRGC). This actor has targeted multiple sectors, including defense,  maritime shipping, transportation, healthcare, and technology. These  operations have frequently relied on watering hole attacks and social  engineering to deliver custom .NET malware. Prior research also identified  custom Crimson Sandstorm malware using email-based command-and-control (C2)  channels. The use of LLMs by Crimson Sandstorm has reflected the broader  behaviors that the security community has observed from this threat actor.  Interactions have involved requests for support around social engineering,  assistance in troubleshooting errors, .NET development, and ways in which an  attacker might evade detection when on a compromised machine. [Microsoft](https://www.microsoft.com/en-us/security/blog/2024/02/14/staying-ahead-of-threat-actors-in-the-age-of-ai/) | **LLM-supported  social engineering**   T1566 - Phishing      **LLM-enhanced scripting techniques**   T1588 - Develop Capabilities: Tool      **LLM-enhanced anomaly detection evasion**   T1562.001 - Impair Defenses: Disable or Modify Tools | link  coming soon                                            |
| Aquatic  Panda                                             | Charcoal  Typhoon, ControlX, RedHotel, Bronze University, Red Scully, Chromium | Chinese  state-affiliated threat actor with a broad operational scope. Activities have  predominantly focused on entities within Taiwan, Thailand, Mongolia,  Malaysia, France, and Nepal, with observed interests extending to  institutions and individuals globally who oppose China’s policies. In recent  operations, this actor group has been observed interacting with LLMs in ways  that suggest a limited exploration of how LLMs can augment their technical  operations. This has consisted of using LLMs to support tooling development,  scripting, understanding various commodity cybersecurity tools, and for  generating content that could be used to social engineer targets. [Microsoft](https://www.microsoft.com/en-us/security/blog/2024/02/14/staying-ahead-of-threat-actors-in-the-age-of-ai/) | **LLM-informed  reconnaissance**   T1588.006 Obtain Capabilities: Vulnerabilities      **LLM-enhanced scripting techniques**   T1587 - Develop Capabilities      **LLM-refined operational command techniques**   TA0003 - Persistence and TA004 Privilege Escalation | [Aquatic   Panda Reports](https://github.com/cybershujin/Threat-Actors-Use-of-Artifical-Intelligence/tree/main/Aquatic Panda) |
| Sodium                                                     | Salmon  Typhoon, Samurai Panda, Maverick Panda, APT4         | Sophisticated  Chinese state-affiliated threat actor with a history of targeting US defense  contractors, government agencies, and entities within the cryptographic  technology sector. This threat actor has demonstrated its capabilities  through the deployment of malware, such as Win32/Wkysol, to maintain remote  access to compromised systems. With over a decade of operations marked by  intermittent periods of dormancy and resurgence. (Sodium's) interactions with  LLMs throughout 2023 appear exploratory and suggest that this threat actor is  evaluating the effectiveness of LLMs in sourcing information on potentially  sensitive topics, high profile individuals, regional geopolitics, US  influence, and internal affairs. This tentative engagement with LLMs could  reflect both a broadening of their intelligence-gathering toolkit and an  experimental phase in assessing the capabilities of emerging  technologies. [Microsoft](https://www.microsoft.com/en-us/security/blog/2024/02/14/staying-ahead-of-threat-actors-in-the-age-of-ai/) | **LLM-informed  reconnaissance**   T1593 - Search Open Websites/Domains      **LLM-enhanced scripting techniques**   T1587.001 - Develop Capabilities: Malware      **LLM-refined operational command techniques**   T1564 - Hide Artifacts      **LLM-Aided technical translation and explanation**   T1593 - Search Open Websites/Domains | link  coming soon                                            |
| Unsub1  - Kaspersky                                        | N/A                                                          | Using  fake sites, they hosted “GPT chats” supposedly capable of diagnosing computer  problems, making money and such. In fact, the site deployed no AI models, but  only used the topic to stir interest with potential victims and make a bigger  killing. One such page mimicking the Microsoft website warned visitors that  their computer was infected with a Trojan. To avoid losing data, they were  advised not to reboot or turn off the device until the issue was resolved.  Two options were offered: call a hotline or chat with Lucy, an AI chatbot. In  the second case, you had to choose a method of diagnosing the device, after  which the bot said it was unable to solve the problem and recommended calling  support. Naturally, professional scammers, not Microsoft engineers, were  waiting at the other end of the line. *Analyst note: This sounds  like [Bazacall](https://www.microsoft.com/en-us/security/blog/2021/07/29/bazacall-phony-call-centers-lead-to-exfiltration-and-ransomware/) like activity* [Source](https://securelist.com/spam-phishing-report-2023/112015/) | **Lookalike  LLM**   T1583 Acquire Infrastructure            | N/A                                                          |
| Unsub2  - Kaspersky                                        | N/A                                                          | “Smart  chatbots” were also used as “consultants” on making money online. On one site  a bot pretending to be an Elon Musk design advertised investment services.  After telling the new “client” that it could make them rich quickly, the  robot asked about their education, income level, and investment experience.  Regardless of the answers, the bot informed the client that it would do all  the earning. Next, it demonstrated an amount it could offer and prompted the  user to register simply by providing their contact details. Events then  likely unfolded as in other similar schemes: The “AI” asked for a small fee  in recognition of its intellectual abilities, and then simply vanished into  the ether. [Source](https://securelist.com/spam-phishing-report-2023/112015/) | **Lookalike  LLM**   T1583 Acquire Infrastructure      **LLM-supported social engineering**   T1566 - Phishing | N/A                                                          |
| Multiple  Unsub - JFrog                                    | N/A                                                          | In  an "investigation of a malicious machine-learning model...The model’s  payload grants the attacker a shell on the compromised machine, enabling them  to gain full control over victims’ machines through what is commonly referred  to as a “backdoor”...It’s crucial to emphasize that when we refer to  “malicious models”, we specifically denote those housing real, harmful  payloads. Our analysis has pinpointed around 100 instances of such models to  date... [Source](https://jfrog.com/blog/data-scientists-targeted-by-malicious-hugging-face-ml-models-with-silent-backdoor/) | **LL  Models used for backdoor deployment**   T1195.001 Supply Chain Compromise - Compromise Software Depedencies and  Development Tools      T1059 Command and Scripting Interpreter | N/A                                                          |
| baller423/goober2                                          | potentially  star23/baller13 or just ties between them       | Recently,  our scanning environment flagged a particularly intriguing PyTorch model  uploaded by a new user named baller423—though since deleted. The repository,  baller423/goober2, contained a PyTorch model file harboring an intriguing  payload....This IP address range belonging to KREOnet, which stands for  “Korea Research Environment Open NETwork,” may serve as potential evidence  suggesting the involvement of researchers in attempting the exploit [Source](https://jfrog.com/blog/data-scientists-targeted-by-malicious-hugging-face-ml-models-with-silent-backdoor/) | **LL  Models used for backdoor deployment**   T1195.001 Supply Chain Compromise - Compromise Software Depedencies and  Development Tools      T1059.001 Command and Scripting Interpreter: Powershell |                                                              |
| star23/baller13                                            | potentially  baller423/goober2 or just ties between them     | Shortly  after the model was removed, we encountered further instances of the same  payload with varying IP addresses. One such instance remains active:  star23/baller13. It’s worth noting the similarity in the model name to the  deleted user, suggesting potential ties between them. [Source](https://jfrog.com/blog/data-scientists-targeted-by-malicious-hugging-face-ml-models-with-silent-backdoor/) | **LL  Models used for backdoor deployment**   T1195.001 Supply Chain Compromise - Compromise Software Depedencies and  Development Tools   T1059.001 Command and Scripting Interpreter: Powershell | N/A                                                          |
| Multiple  Unsub - NCSC UK                                  | N/A                                                          | Threat  actors, including ransomware actors, are already using AI to increase the  efficiency and effectiveness of aspects of cyber operations, such as  reconnaissance, phishing and coding [Source](https://www.ncsc.gov.uk/report/impact-of-ai-on-cyber-threat) | **LLM-informed  reconnaissance**   T1593 - Search Open Websites/Domains      **LLM-supported social engineering**   T1566 - Phishing      **LLM-aided development**   T1587 - Develop Capabilities: Tool      **LLM-aided development**   T1587 - Develop Capabilities: Malware      **LLM-enhanced scripting techniques**   T1587 - Develop Capabilities | [Multiple   Unsub - NCSC](https://github.com/cybershujin/Threat-Actors-Use-of-Artifical-Intelligence/tree/main/Multiple UnSub SlashNext) |
| Multiple  Unsub - SlashNext                                | N/A                                                          | Since  Q4 of 2022 when ChatGPT became widely available, there has been a 1,265%  increase in malicious phishing emails, with a 967% rise in credential  phishing in particular. [Source](https://www.cnbc.com/2023/11/28/ai-like-chatgpt-is-creating-huge-increase-in-malicious-phishing-email.html) | **LLM-supported  social engineering**   T1566 - Phishing     | [Multile   Unsub - SlashNext](https://github.com/cybershujin/Threat-Actors-Use-of-Artifical-Intelligence/tree/main/Multiple UnSub SlashNext) |
| Multiple  Unsub North Korea - US National Security Advisor | N/A                                                          | On  Oct. 18, 2023 U.S. Deputy National Security Advisor Anne Neuberger said that  North Korea's use of artificial intelligence (AI) is enhancing the country's  cyber capabilities, which puts enterprises around the globe at significant  risk. Neuberger said, "We have observed some North Korean and other  nation-state and criminal actors try to use AI models to help accelerate  writing malicious software and finding systems to exploit." | **LLM-assisted  vulnerability research**   T1588.006 Obtain Capabilities: Vulnerabilities      **LLM-enhanced scripting techniques**   T1588 - Develop Capabilities: Tool |                                                              |
| Scattered  Spider                                          | UNC3944,  Storm-0875                                         | In  the second half of 2023, SCATTERED SPIDER used the Azure AD PowerShell module  to download all Entra ID user immutable IDs at a North American financial  services victim. Using its Entra ID backdoor, the adversary could log in as  any of the downloaded users. The PowerShell used to download the users’  immutable IDs resembled large language model (LLM) outputs such as those from  ChatGPT. In particular, the pattern of one comment, the actual command and  then a new line for each command matches the Llama 2 70B model output. [Source](https://asantecloud.com/wp-content/uploads/2024/02/GlobalThreatReport2024_Asante.pdf) | **LLM-enhanced  scripting techniques**   T1588 - Develop Capabilities: Tool | Link  Coming Soon                                            |
| Indrik  Spider                                             | Evil  Corp                                                   | In  February 2023, CrowdStrike Services responded to an INDRIK SPIDER incident  involving BITWISE SPIDER’s LockBit RED ransomware. During this incident,  INDRIK SPIDER exfiltrated credentials from cloud-based credential manager  Azure Key Vault. Logs show that INDRIK SPIDER also visited ChatGPT while  interacting with the Azure Portal. In addition to visiting ChatGPT while  browsing the Azure Portal — presumably to understand how to navigate in Azure  — browsing activity analysis indicates INDRIK SPIDER used search engines such  as Google and Bing and searched on GitHub during the operations to understand  how to exfiltrate Azure Key Vault credentials. Using search engines and  visiting ChatGPT indicate that though INDRIK SPIDER is likely new to the  cloud and not yet sophisticated in this domain, it is using generative AI to  fill these knowledge gaps. [Source](https://asantecloud.com/wp-content/uploads/2024/02/GlobalThreatReport2024_Asante.pdf) | **LLM-informed  reconnaissance**   T1593 - Search Open Websites/Domains      [Source](https://www.state.gov/digital-press-briefing-with-anne-neuberger-deputy-national-security-advisor-for-cyber-and-emerging-technologies/) | N/A                                                          |
| UnSubs  - Mandiant                                         | N/A  but described as "actors aligned with nation-states including Russia,  the People's Republic of China (PRC), Iran, Ethiopia, Indonesia, Cuba,  Argentina, Mexico, Ecuador, and El Salvador, along with non-state actors such  as individuals on the 4chan forum." | Since  2019, Mandiant has identified numerous instances of information operations  leveraging GANs, typically for use in profile photos of inauthentic personas,  including by actors aligned with nation-states including Russia, the People's  Republic of China (PRC), Iran, Ethiopia, Indonesia, Cuba, Argentina, Mexico,  Ecuador, and El Salvador, along with non-state actors such as individuals on  the 4chan forum. We judge that the publicly available nature of GAN-generated  image tools such as the website thispersondoesnotexist.com has likely  contributed to their frequent usage in information operations (Figure 2).  Actors have also taken steps to obfuscate the AI-generated origin of their  profile photos through tactics like adding filters or retouching facial  features...Mandiant has noted evidence of financially motivated actors using  manipulated video and voice content in business email compromise (BEC) scams,  North Korean cyber espionage actors using manipulated images to defeat know  your customer (KYC) requirements, and voice changing technology used in  social engineering targeting Israeli soldiers. [Source](https://www.mandiant.com/resources/blog/threat-actors-generative-ai-limited) | **DeepFake  for Impersonation**   T1587 - Develop capabilities | N/A                                                          |

**Appendix A: LLM-themed TTPs**

**LLM-informed reconnaissance:** Employing LLMs to gather actionable intelligence on technologies and potential vulnerabilities. (CREDIT: Microsoft)

 **LLM-enhanced scripting techniques:** Utilizing LLMs to generate or refine scripts that could be used in cyberattacks, or for basic scripting tasks such as programmatically identifying certain user events on a system and assistance with troubleshooting and understanding various web technologies.(CREDIT: Microsoft)

 **LLM-aided development:** Utilizing LLMs in the development lifecycle of tools and programs, including those with malicious intent, such as malware.(CREDIT: Microsoft)

 **LLM-supported social engineering:** Leveraging LLMs for assistance with translations and communication, likely to establish connections or manipulate targets.(CREDIT: Microsoft)

 **LLM-assisted vulnerability research:** Using LLMs to understand and identify potential vulnerabilities in software and systems, which could be targeted for exploitation.(CREDIT: Microsoft)

 **LLM-optimized payload crafting:** Using LLMs to assist in creating and refining payloads for deployment in cyberattacks.(CREDIT: Microsoft)

 **LLM-enhanced anomaly detection evasion:** Leveraging LLMs to develop methods that help malicious activities blend in with normal behavior or traffic to evade detection systems.(CREDIT: Microsoft)

 **LLM-directed security feature bypass:** Using LLMs to find ways to circumvent security features, such as two-factor authentication, CAPTCHA, or other access controls.(CREDIT: Microsoft)

 **LLM-advised resource development:** Using LLMs in tool development, tool modifications, and strategic operational planning.(CREDIT: Microsoft)

 **Lookalike LLM:** Using LLMs to appear to be another legitimate LLM tool such as a chatbot. Similar to look-alike domain activity. (CREDIT: Rachel James based on Kaspersky report)

 **LL Models used for backdoor deployment:** The model’s payload grants the attacker a shell on the compromised machine, enabling them to gain full control over victims’ machines through what is commonly referred to as a “backdoor” (CREDIT: Rachel James based on JFrog report)

**DeepFake for Impersonation:** Where generative AI is used to make audio, video or photographic media used to impersonate individuals (CREDIT: Rachel James based on various reports)



## Deepfake categories

**DeepFake for Impersonation:** Where generative AI is used to make audio, video or photographic media used to impersonate individuals (CREDIT: Rachel James based on various reports)

- Fraud
- Hiring

**DeepFake for Synthetic Identity:** Where generative AI is used to make audio, video or photographic media used to impersonate individuals (CREDIT: Rachel James based on various reports)





 **Sources for the above list**

**2024 Reports**

| **Month** | **Org**                         | **Link**                                                     |
| --------- | ------------------------------- | ------------------------------------------------------------ |
| Janurary  | National  Cyber Security Centre | [The   near-term impact of AI on the cyber threat](https://www.ncsc.gov.uk/report/impact-of-ai-on-cyber-threat#section_5) - *Analyst  note: one of the most sensible and non-sensationalist works out there on the  topic* |
| Feburary  | Microsoft                       | [Staying   ahead of threat actors in the age of AI](https://www.microsoft.com/en-us/security/blog/2024/02/14/staying-ahead-of-threat-actors-in-the-age-of-ai/) |
| Feburary  | Microsoft                       | [Cyber   Signals Issue 6](https://www.microsoft.com/en-us/security/blog/2024/02/14/staying-ahead-of-threat-actors-in-the-age-of-ai/](https:/www.microsoft.com/en-us/security/business/security-insider/wp-content/uploads/2024/02/cyber-signals-issue-6.pdf)) |
| Feburary  | OpenAI                          | [Disrupting   malicious uses of AI by state-affiliated threat actors](https://openai.com/blog/disrupting-malicious-uses-of-ai-by-state-affiliated-threat-actors) |
| Feburary  | JFrog                           | [Malicious   AI models on Hugging Face backdoor users’ machines](https://jfrog.com/blog/data-scientists-targeted-by-malicious-hugging-face-ml-models-with-silent-backdoor/) |
| March     | Tripwire                        | [Cybersecurity   in the Age of AI: Exploring AI-Generated Cyber Attacks](https://www.tripwire.com/state-of-security/cybersecurity-age-ai-exploring-ai-generated-cyber-attacks) |
| March     | Kaspersky                       | [Spam   and phishing in 2023](https://securelist.com/spam-phishing-report-2023/112015/) |

**2023 Reports**

| **Month** | **Org**    | **Link**                                                     |
| --------- | ---------- | ------------------------------------------------------------ |
| June      | FBI  / IC3 | [Malicious Actors   Manipulating Photos and Videos to Create Explicit Content and Sextortion   Schemes](https://www.ic3.gov/Media/Y2023/PSA230605) |
| July      | SlashNext  | [WormGPT   – The Generative AI Tool Cybercriminals Are Using to Launch Business Email   Compromise Attacks](https://slashnext.com/blog/wormgpt-the-generative-ai-tool-cybercriminals-are-using-to-launch-business-email-compromise-attacks/) |
| November  | SlashNext  | [AI   tools such as ChatGPT are generating a mammoth increase in malicious phishing   emails](https://www.cnbc.com/2023/11/28/ai-like-chatgpt-is-creating-huge-increase-in-malicious-phishing-email.html) |
| August    | Mandiant   | [Threat   Actors are Interested in Generative AI, but Use Remains Limited](https://www.mandiant.com/resources/blog/threat-actors-generative-ai-limited) |
